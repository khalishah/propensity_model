{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.31.1\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except:\n",
    "    !pip install tqdm\n",
    "    from tqdm import tqdm\n",
    "import numpy as np\n",
    "# from sklearn.decomposition import IncrementalPCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sns.set(rc={'figure.figsize':(20,10)})\n",
    "pd.set_option('display.max_columns', 100)\n",
    "sns.set(font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = \"el_train/\"\n",
    "filenames = [[data_dir+x] for x in os.listdir(data_dir) if \"csv\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.read_csv(filenames[0][0], nrows=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_read = list(pd.read_csv(filenames[0][0], nrows=1).columns)\n",
    "columnsToUse = list(pd.read_csv(filenames[0][0], nrows=1).columns)\n",
    "\n",
    "columnsToUse.pop(columnsToUse.index(\"memshpno\"))\n",
    "columnsToUse.pop(columnsToUse.index(\"target_month\"))\n",
    "\n",
    "labelColumn = \"label\"\n",
    "recordDefaults = [[0.0] for col in columnsToUse]\n",
    "\n",
    "select_cols = [i for i,x in enumerate(pd.read_csv(filenames[0][0], nrows=1).columns) if x in columnsToUse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = np.array(columnsToUse[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeseries\n",
    "# Aggregate to month\n",
    "# Category Specifc/Brand Specific\n",
    "\n",
    "# Try both Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44255"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_size = 1024\n",
    "\n",
    "total_examples = 442553\n",
    "\n",
    "test_examples = total_examples//10\n",
    "\n",
    "test_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filenames, b_size=1024, split=10000):\n",
    "    def decode_csv(value_column):\n",
    "        columns = tf.decode_csv(value_column, record_defaults=recordDefaults, select_cols=select_cols)\n",
    "        ts = columns[:-1]\n",
    "        label = columns[-1]\n",
    "        return ts, label\n",
    "\n",
    "    # Create dataset from file list\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    dataset = dataset.flat_map(lambda filename: tf.data.TextLineDataset(filename).skip(1))\n",
    "    dataset = dataset.map(decode_csv)  # Transform each elem by applying decode_csv fn\n",
    "\n",
    "    if True:\n",
    "        num_epochs = None # indefinitely\n",
    "    else:\n",
    "        num_epochs = 1 # end-of-input after this\n",
    "\n",
    "    dataset = dataset.shuffle(10240)\n",
    "    test_data = dataset.take(split)\n",
    "    train_data = dataset.skip(split)\n",
    "\n",
    "    test_data = test_data.repeat(num_epochs).batch(b_size)\n",
    "    train_data = train_data.repeat(num_epochs).batch(b_size)\n",
    "\n",
    "    test_iterator = test_data.make_one_shot_iterator()\n",
    "    train_iterator = train_data.make_one_shot_iterator()\n",
    "\n",
    "    def make_gen_from_iterator(osi):\n",
    "        sess = tf.Session()\n",
    "        next_val = osi.get_next()\n",
    "        while True:\n",
    "            with sess.as_default():\n",
    "                ts, labels = sess.run(next_val)\n",
    "                ts = np.reshape(ts, newshape=(-1,12,6))\n",
    "                yield ts, labels\n",
    "\n",
    "    test_gen = make_gen_from_iterator(test_iterator)\n",
    "    train_gen = make_gen_from_iterator(train_iterator)\n",
    "    \n",
    "    return train_gen, test_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen, test_gen = read_dataset(filenames=filenames, b_size=b_size, split=test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = train_gen.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 12, 6)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_gen.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Train Set Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_size = total_examples - total_examples//10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "388"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_steps_per_epoch = train_set_size//b_size\n",
    "train_steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Test Set Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_size = total_examples//10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_steps_per_epoch = test_set_size//b_size\n",
    "test_steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the recurrent neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing relevant libraries\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import Model\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense ,GRU, Embedding, Flatten, RepeatVector, TimeDistributed, Dropout, Bidirectional, Reshape, concatenate\n",
    "from tensorflow.python.keras.optimizers import RMSprop, Adam, Nadam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.python.keras.initializers import RandomUniform\n",
    "from tensorflow.python.keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Keras API for its simplicity. The type of recurrent neural network used is called a gated recurrent unit (GRU). It will have 300 outputs for each timestep. The simple GRU will be wrapped inside a Bidirectional layer. Bidirectional layer creates two GRUs, in which one GRU traverses the signal in reverse order. Both of these GRUs will output 300 values at each timestep which are summed together at the end because \"merge_mode = True\" is set.\n",
    "\n",
    "The last layer would be a dense layer with a \"linear\" activation function due to intention of predicting a continuous range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ts_model():\n",
    "    # Creating the Optimizer\n",
    "    #optimizer = Adam(lr=1e-2)\n",
    "    optimizer = Adam()\n",
    "\n",
    "    ts_input = Input(shape=(12,6), dtype='float32', name='main_input')\n",
    "\n",
    "    # A LSTM will transform the vector sequence into a single vector,\n",
    "    # containing information about the entire sequence\n",
    "    gru_1 = Bidirectional(GRU(64, return_sequences = True))(ts_input)\n",
    "    gru_2 = Bidirectional(GRU(64, return_sequences = True))(gru_1)\n",
    "    gru_flattened_ts = Flatten()(gru_2)\n",
    "    \n",
    "    x = Dense(8, activation='relu')(gru_flattened_ts)\n",
    "#     x = Dense(8, activation='relu')(x)\n",
    "\n",
    "    # And finally we add the main logistic regression layer\n",
    "    output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=[ts_input], outputs=[output])\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback functions\n",
    "\n",
    "Callback functions are creating for logging the progress of training in Tensorboard and saving the best performing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the Checkpoint File name and path\n",
    "path_checkpoint = 'ckpt_model_ts.keras'\n",
    "\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "\n",
    "# Callback for early stopping i-e if the model starts to overfit\n",
    "callback_early_stopping = EarlyStopping(monitor='val_acc',\n",
    "                                        patience=4, verbose=1)\n",
    "\n",
    "# Callback to reduce learning rate if the learning stangnates\n",
    "callback_reduce_lr = ReduceLROnPlateau(monitor='val_acc',\n",
    "                                       factor=0.1,\n",
    "                                       min_lr=1e-4,\n",
    "                                       patience=2,\n",
    "                                       verbose=1)\n",
    "\n",
    "# Callback for logging prgress in tensorboard\n",
    "callback_tensorboard = TensorBoard(log_dir='./model/log/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=True)\n",
    "\n",
    "callbacks = [\n",
    "#     callback_early_stopping,\n",
    "             callback_checkpoint,\n",
    "             callback_reduce_lr,\n",
    "             callback_tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {0: 1.,\n",
    "                1: 2.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = get_ts_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 12, 6)             0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 12, 128)           27264     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 12, 128)           74112     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 12296     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 113,681\n",
      "Trainable params: 113,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "43/43 [==============================] - 15s 346ms/step - loss: 0.3052 - acc: 0.8952\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.30518, saving model to ckpt_model_ts.keras\n",
      "388/388 [==============================] - 227s 585ms/step - loss: 0.3159 - acc: 0.8895 - val_loss: 0.3052 - val_acc: 0.8952\n",
      "Epoch 2/10\n",
      "43/43 [==============================] - 45s 1s/step - loss: 0.3043 - acc: 0.8958\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.30518 to 0.30427, saving model to ckpt_model_ts.keras\n",
      "388/388 [==============================] - 299s 771ms/step - loss: 0.3071 - acc: 0.8955 - val_loss: 0.3043 - val_acc: 0.8958\n",
      "Epoch 3/10\n",
      "43/43 [==============================] - 62s 1s/step - loss: 0.3032 - acc: 0.8968\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.30427 to 0.30319, saving model to ckpt_model_ts.keras\n",
      "388/388 [==============================] - 887s 2s/step - loss: 0.3061 - acc: 0.8957 - val_loss: 0.3032 - val_acc: 0.8968\n",
      "Epoch 4/10\n",
      "43/43 [==============================] - 16s 363ms/step - loss: 0.3033 - acc: 0.8965\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.30317\n",
      "388/388 [==============================] - 887s 2s/step - loss: 0.3031 - acc: 0.8966 - val_loss: 0.3033 - val_acc: 0.8965\n",
      "Epoch 7/10\n",
      "43/43 [==============================] - 14s 326ms/step - loss: 0.3026 - acc: 0.8968\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.30317 to 0.30257, saving model to ckpt_model_ts.keras\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "388/388 [==============================] - 893s 2s/step - loss: 0.3033 - acc: 0.8965 - val_loss: 0.3026 - val_acc: 0.8968\n",
      "Epoch 8/10\n",
      "43/43 [==============================] - 15s 348ms/step - loss: 0.3021 - acc: 0.8960\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.30257 to 0.30213, saving model to ckpt_model_ts.keras\n",
      "388/388 [==============================] - 895s 2s/step - loss: 0.3029 - acc: 0.8967 - val_loss: 0.3021 - val_acc: 0.8960\n",
      "Epoch 9/10\n",
      "43/43 [==============================] - 14s 322ms/step - loss: 0.3025 - acc: 0.8963\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.30213\n",
      "388/388 [==============================] - 896s 2s/step - loss: 0.3028 - acc: 0.8967 - val_loss: 0.3025 - val_acc: 0.8963\n",
      "Epoch 10/10\n",
      "43/43 [==============================] - 14s 320ms/step - loss: 0.3027 - acc: 0.8964\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.30213\n",
      "388/388 [==============================] - 896s 2s/step - loss: 0.3028 - acc: 0.8966 - val_loss: 0.3027 - val_acc: 0.8964\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator=train_gen,\n",
    "                    epochs=10,\n",
    "#                     run_tensorboard_locally=True,\n",
    "                    steps_per_epoch=train_steps_per_epoch,\n",
    "#                     class_weight=class_weights,\n",
    "                    validation_data=test_gen,\n",
    "                    validation_steps=test_steps_per_epoch, \n",
    "                    callbacks = callbacks,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:12<00:00,  3.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# Validation producing Confusion Variables\n",
    "tp, tn, fp, fn = 0,0,0,0\n",
    "tot_true = 0\n",
    "\n",
    "for i in tqdm(range(test_steps_per_epoch)):\n",
    "    batch_in, y_true = test_gen.next()\n",
    "\n",
    "    y_true = y_true.reshape((b_size,1))\n",
    "    tot_true+=y_true.sum()\n",
    "    y_pred = model.predict_on_batch(batch_in)\n",
    "    y_pred_lab = (y_pred >= 0.5).astype(\"int\")\n",
    "    \n",
    "    corr = (y_pred_lab == y_true.reshape(y_pred_lab.shape))\n",
    "\n",
    "    tp += y_true[corr].sum()\n",
    "    tn += y_true[corr].size - y_true[corr].sum()\n",
    "\n",
    "    fn += y_true[~corr].sum()\n",
    "    fp += y_true[~corr].size - y_true[~corr].sum()\n",
    "    \n",
    "\n",
    "tot_neg = (b_size*test_steps_per_epoch) - tot_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1:1 Weights 5 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38687.0, 288.0, 38975.0)\n",
      "(4280.0, 777.0, 5057.0)\n"
     ]
    }
   ],
   "source": [
    "print(tn, fp, tot_neg)\n",
    "print(fn, tp, tot_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Class Precision: 0.729577464789\n",
      "Positive Class Recall   : 0.153648408147\n"
     ]
    }
   ],
   "source": [
    "pos_rec = tp/(tp+fn)# Recall\n",
    "pos_prc = tp/(tp+fp)# Precision\n",
    "\n",
    "print(\"Positive Class Precision: {}\".format(pos_prc))\n",
    "print(\"Positive Class Recall   : {}\".format(pos_rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Class Precision: 0.900388670375\n",
      "Negative Class Recall   : 0.992610647851\n"
     ]
    }
   ],
   "source": [
    "neg_rec = tn/(tn+fp)# Recall\n",
    "neg_prc = tn/(tn+fn)# Precision\n",
    "\n",
    "print(\"Negative Class Precision: {}\".format(neg_prc))\n",
    "print(\"Negative Class Recall   : {}\".format(neg_rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model_el_10_epochs.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"m3_data/el_val/\"\n",
    "val_filenames = [[data_dir+x] for x in os.listdir(data_dir) if \"csv\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_read = list(pd.read_csv(filenames[0][0], nrows=1).columns)\n",
    "columnsToUse = list(pd.read_csv(filenames[0][0], nrows=1).columns)\n",
    "\n",
    "columnsToUse.pop(columnsToUse.index(\"memshpno\"))\n",
    "columnsToUse.pop(columnsToUse.index(\"target_month\"))\n",
    "\n",
    "labelColumn = \"label\"\n",
    "recordDefaults = [[0.0] for col in columnsToUse]\n",
    "\n",
    "select_cols = [i for i,x in enumerate(pd.read_csv(filenames[0][0], nrows=1).columns) if x in columnsToUse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_size = 1024\n",
    "\n",
    "set_size = 39588"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_validation_dataset(filenames, b_size=1024):\n",
    "    def decode_csv(value_column):\n",
    "        columns = tf.decode_csv(value_column, record_defaults=recordDefaults, select_cols=select_cols)\n",
    "        ts = columns[:-1]\n",
    "        label = columns[-1]\n",
    "        return ts, label\n",
    "\n",
    "    # Create dataset from file list\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    dataset = dataset.flat_map(lambda filename: tf.data.TextLineDataset(filename).skip(1))\n",
    "    dataset = dataset.map(decode_csv)  # Transform each elem by applying decode_csv fn\n",
    "\n",
    "    if True:\n",
    "        num_epochs = None # indefinitely\n",
    "    else:\n",
    "        num_epochs = 1 # end-of-input after this\n",
    "\n",
    "    test_data = dataset.shuffle(10000)\n",
    "\n",
    "    test_data = test_data.repeat(num_epochs).batch(b_size)\n",
    "\n",
    "    test_iterator = test_data.make_one_shot_iterator()\n",
    "\n",
    "    def make_gen_from_iterator(osi):\n",
    "        sess = tf.Session()\n",
    "        next_val = osi.get_next()\n",
    "        while True:\n",
    "            with sess.as_default():\n",
    "                ts, labels = sess.run(next_val)\n",
    "                ts = np.reshape(ts, newshape=(-1,12,6))\n",
    "                yield ts, labels\n",
    "\n",
    "    test_gen = make_gen_from_iterator(test_iterator)\n",
    "    \n",
    "    return test_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen = read_validation_dataset(filenames=val_filenames, b_size=b_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = val_gen.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_steps_per_epoch = set_size//b_size\n",
    "val_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_ts_model()\n",
    "model.load_weights(\"model_el_10_epochs.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:12<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# Validation producing Confusion Variables\n",
    "tp, tn, fp, fn = 0,0,0,0\n",
    "tot_true = 0\n",
    "\n",
    "for i in tqdm(range(val_steps_per_epoch)):\n",
    "    batch_in, y_true = val_gen.next()\n",
    "\n",
    "    y_true = y_true.reshape((b_size,1))\n",
    "    tot_true+=y_true.sum()\n",
    "    y_pred = model.predict_on_batch(batch_in)\n",
    "    y_pred_lab = (y_pred >= 0.5).astype(\"int\")\n",
    "    \n",
    "    corr = (y_pred_lab == y_true.reshape(y_pred_lab.shape))\n",
    "\n",
    "    tp += y_true[corr].sum()\n",
    "    tn += y_true[corr].size - y_true[corr].sum()\n",
    "\n",
    "    fn += y_true[~corr].sum()\n",
    "    fp += y_true[~corr].size - y_true[~corr].sum()\n",
    "    \n",
    "tot_neg = (b_size*val_steps_per_epoch) - tot_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32265.0, 198.0, 32463.0)\n",
      "(5806.0, 643.0, 6449.0)\n"
     ]
    }
   ],
   "source": [
    "print(tn, fp, tot_neg)\n",
    "print(fn, tp, tot_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Predictions: 841.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive Predictions: {}\".format(fp+tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Class Precision: 0.764565992866\n",
      "Positive Class Recall   : 0.0997053806792\n"
     ]
    }
   ],
   "source": [
    "pos_rec = tp/(tp+fn)# Recall\n",
    "pos_prc = tp/(tp+fp)# Precision\n",
    "\n",
    "print(\"Positive Class Precision: {}\".format(pos_prc))\n",
    "print(\"Positive Class Recall   : {}\".format(pos_rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Class Precision: 0.847495468992\n",
      "Negative Class Recall   : 0.993900748544\n"
     ]
    }
   ],
   "source": [
    "neg_rec = tn/(tn+fp)# Recall\n",
    "neg_prc = tn/(tn+fn)# Precision\n",
    "\n",
    "print(\"Negative Class Precision: {}\".format(neg_prc))\n",
    "print(\"Negative Class Recall   : {}\".format(neg_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating and Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"m3_data/el_val/\"\n",
    "val_filenames = [[data_dir+x] for x in os.listdir(data_dir) if \"csv\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_read = list(pd.read_csv(filenames[0][0], nrows=1).columns)\n",
    "columnsToUse = list(pd.read_csv(filenames[0][0], nrows=1).columns)\n",
    "\n",
    "# columnsToUse.pop(columnsToUse.index(\"target_month\"))\n",
    "\n",
    "labelColumn = \"label\"\n",
    "recordDefaults = [[0.0] for col in columnsToUse]\n",
    "recordDefaults[0] = [\"\"]\n",
    "select_cols = [i for i,x in enumerate(pd.read_csv(filenames[0][0], nrows=1).columns) if x in columnsToUse]\n",
    "\n",
    "def read_dataset_w_memshpno(filenames, b_size=1024):\n",
    "    def decode_csv(value_column):\n",
    "        columns = tf.decode_csv(value_column, record_defaults=recordDefaults, select_cols=select_cols)\n",
    "        memshpno = columns[0]\n",
    "        ts = columns[1:-2]\n",
    "        month = columns[-2]\n",
    "        label = columns[-1]\n",
    "        return memshpno, ts, month, label\n",
    "\n",
    "    # Create dataset from file list\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    dataset = dataset.flat_map(lambda filename: tf.data.TextLineDataset(filename).skip(1))\n",
    "    dataset = dataset.map(decode_csv)  # Transform each elem by applying decode_csv fn\n",
    "\n",
    "    if True:\n",
    "        num_epochs = None # indefinitely\n",
    "    else:\n",
    "        num_epochs = 1 # end-of-input after this\n",
    "\n",
    "    test_data = dataset.shuffle(10000)\n",
    "\n",
    "    test_data = test_data.repeat(num_epochs).batch(b_size)\n",
    "\n",
    "    test_iterator = test_data.make_one_shot_iterator()\n",
    "\n",
    "    def make_gen_from_iterator(osi):\n",
    "        sess = tf.Session()\n",
    "        next_val = osi.get_next()\n",
    "        while True:\n",
    "            with sess.as_default():\n",
    "                memshpno, ts, month, labels = sess.run(next_val)\n",
    "                ts = np.reshape(ts, newshape=(b_size,12,6))\n",
    "                yield memshpno, ts, month, labels\n",
    "\n",
    "    test_gen = make_gen_from_iterator(test_iterator)\n",
    "    \n",
    "    return test_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_size = 0\n",
    "for f in val_filenames:\n",
    "    set_size += pd.read_csv(f[0]).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39588"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,1000):\n",
    "    if set_size%i==0:\n",
    "        b_size = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3299"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b_size = 12\n",
    "# set_size = 39588\n",
    "val_steps_per_epoch = set_size//b_size\n",
    "val_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = read_dataset_w_memshpno(val_filenames, b_size=b_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_ts_model()\n",
    "model.load_weights(\"model_el_10_epochs.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3299/3299 [00:52<00:00, 62.63it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(val_steps_per_epoch)):\n",
    "    memshpnos, batch_in, month, y_true = data_iterator.next()\n",
    "    y_pred = model.predict_on_batch(batch_in)\n",
    "    y_pred = np.reshape(y_pred, (b_size,))\n",
    "    \n",
    "    batch_df = pd.DataFrame(data=np.column_stack((memshpnos, y_true, y_pred, month)), columns=(\"memshpno\", \"y_true\", \"y_pred\", \"month\"))\n",
    "    \n",
    "    batch_df[\"jan\"] = (batch_df[\"month\"]==25).astype(int)\n",
    "    batch_df[\"feb\"] = (batch_df[\"month\"]==26).astype(int)\n",
    "\n",
    "    batch_df.drop([\"month\"], axis=1, inplace=True)\n",
    "    \n",
    "    with open(\"final_m3_el_4.csv\", \"a\") as m3_file:\n",
    "        batch_df.to_csv(m3_file, index=False, header=True if i==0 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_m3_el_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "bought_df = df[df['y_true']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6553, 5)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bought_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5302, 5)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bought_df.drop_duplicates(\"memshpno\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(650, 5)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bought_df[bought_df[\"y_pred\"]>0.5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bought_df.drop([\"y_true\", \"y_pred\"], axis=1).to_csv(\"month_labels.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
